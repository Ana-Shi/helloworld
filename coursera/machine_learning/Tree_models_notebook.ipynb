{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 优点： \n",
    "\n",
    "      易理解  \n",
    "\n",
    "      可以识别最重要的变量  \n",
    "    \n",
    "      数据的异常值和缺失值没关系  \n",
    "        \n",
    "      可以解决分类和回归问题\n",
    "      \n",
    "      非参数模型，决策树对数据空间分布和分类器结构没有任何假设。\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 缺点：  \n",
    "\n",
    "    容易过拟合，可以通过参数调整和剪枝遏制过拟合。  \n",
    "    \n",
    "    不太适合连续性的变量，容易丢失信息\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分类树和回归树   \n",
    "\n",
    "    回归树：当一个数据落在某个叶子结点时，则预测值是该叶子结点的平均值  \n",
    "    \n",
    "    分类树：是该叶子结点的值  \n",
    "    \n",
    "    树遵循自顶向下的贪婪方法，称为递归二进制分割，每一次分割只关心当前的分割（只寻找可用的最佳变量）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类树：如何决定由哪个节点split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gini系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30人，有15 人玩棒球。特征：性别（男、女），年级（1/2）  \n",
    "预测一个人是否玩棒球  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> gini系数只适用于分类问题，只适用于二分类特征，越高代表分的节点同质性越高。CART数是使用gini系数建立的。  \n",
    "计算公式：  \n",
    "\n",
    "         左子节点gini系数：Left_gini = PR(Positive ratio)^2 + NR(Negative ratio)^2  \n",
    "         \n",
    "         右子节点gini系数：right_gini =  PR(Positive ratio)^2 + NR(Negative ratio)^2  \n",
    "         \n",
    "         该特征在该次split的gini系数是：(LN(左子节点分割的人数)/父节点的人数*Left_gini+RN(右子节点分割的人数)//父节点的人数*right_gini)  \n",
    "         因此该点在gini系数最大的特征上split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  \n",
    "\n",
    "\n",
    "Chi square 计算子节点和父节点之间差异的统计显著性的算法。  \n",
    "适用于分类问题， 可以split到2个或多个子节点。  \n",
    "值越高，子节点和父节点差异的统计显著性越高。\n",
    "CHAID树使用这个Chisquaresplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "\n",
    "\n",
    "\n",
    "计算方式：     \n",
    "\n",
    "由female特征 split：  \n",
    "\n",
    "左节点10人，split之后2人是玩棒球，8人不玩。  \n",
    "\n",
    "右节点20人，split之后13人玩棒球，7人不玩。  \n",
    "\n",
    "左节点的期望值： 玩棒球5人，不玩棒球5人。  \n",
    "右节点的期望值：玩棒球10人，不玩棒球10人。  \n",
    "\n",
    "左节点的玩棒球的LP_Chi_square = ((2-5)^2/5)^1/2  \n",
    "左节点的不玩棒球的LNP_Chi_square = ((8-5)^2/5)^1/2    \n",
    "左节点的玩棒球的RP_Chi_square = ((13-10)^2/10)^1/2  \n",
    "左节点的不玩棒球的RNP_Chi_square = ((7-10)^2/10)^1/2   \n",
    "Chi_square = LP_Chi_square+LNP_Chi_square+RP_Chi_square+RNP_Chi_square \n",
    "这代表由这个特征split之后子节点的纯度变化有多大，这个特征有多有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j计算方式：  \n",
    "父节点的熵 = -（15/30）log2(15/30) - (15/30)log2(15/30)=1,代表这不个impure的节点  \n",
    "left node entropy = -(2/10)log2(2/10)-(8/10)log2(8/10)=0.72  \n",
    "right_node_entropy = -(13/20)log2(13/20) - (7/20)log2(7/20)  =0.93  \n",
    "split_node = 10/30*0.72 + 20/30*0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归树的划分特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction in Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 这种方式使用标准方差公式来选择最佳分割，方差最小的特征为该次分割的特征.  有点像classification_metrics中的decile score,都是与真实均值的靠近程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "\n",
    "Y_mean = sum(Y)/len(Y)  \n",
    "\n",
    "Variance = sum(Y_nxt - Y_mean)^2/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "\n",
    "父节点 mean_value = （15\\*1+15*0）/30 = 0.5    \n",
    "\n",
    "      variance = ((1-0.5)^2*15 +(0-0.5)^2*15)/30  \n",
    "      \n",
    "左子节点 Left_mean_value = (2*1+8*0)/10 = 0.2  \n",
    "\n",
    "        Left_variance = (2*(1-0.2)^2+8*(0-0.2)^2)/10 = 0.16  \n",
    "        \n",
    "右子节点 right_mean_value = (13*1+7*0)/20=0.65  \n",
    "\n",
    "       right_variance = (13*(1-0.65)^2+7*(0-0.65)^2)/20 =0.23    \n",
    "       \n",
    "         \n",
    "variance for split gender = 10/30*0.16+20/30*0.23=0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 树模型的关键参数和抑制过拟合的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树容易过拟合，最坏的情况下，每条训练数据会分到一个节点。  \n",
    "防止过拟合的两种方法：  \n",
    "                 设置树的规模  \n",
    "                 Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 树模型的关键参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_features: 每次分割考虑的最大特征数   \n",
    "\n",
    "        搜索最佳分割时要考虑的特性的数量。这些是随机选择的。  \n",
    "\n",
    "        取特性总数的平方根非常有效，但是我们应该检查到特性总数的30-40%。  \n",
    "\n",
    "        较高的值可能导致过度拟合，但这取决于具体情况。\n",
    "\n",
    "min_sample_split(minimum samples for a node split):   \n",
    "\n",
    "    if ==70, 那么任何node里少于70，那么不能再次split     \n",
    "\n",
    "    定义节点split需要的最小样本数(或观察数)。  \n",
    "\n",
    "    可以用于控制过拟合。较高的值可能会限制模型的学习程度。  \n",
    "\n",
    "    过高的值可能导致不适合，因此，应该使用CV进行调优。\n",
    "    \n",
    "max_depth（Maximum depth of tree (vertical depth)）:   \n",
    "\n",
    "        if ==2,最多split 2 层，不能再次split   \n",
    "        \n",
    "        树的最大深度。  \n",
    "        \n",
    "        用于控制过拟合。  \n",
    "        \n",
    "        应该使用CV进行调优。\n",
    "    \n",
    "min_sample_leaf（Minimum samples for a terminal node (leaf)）:     \n",
    "\n",
    "       if==30, 叶子节点不能少于30个人   \n",
    "    \n",
    "      定义叶子结点中所需的最小样本(或观察值)。  \n",
    "    \n",
    "      用于控制过拟合，类似于min_samples_split。  \n",
    "    \n",
    "      一般来说，对于不平衡的分类问题，应该选择较低的值。  \n",
    "    \n",
    "Maximum number of terminal nodes：  \n",
    "\n",
    "    The maximum number of terminal nodes or leaves in a tree.  \n",
    "    \n",
    "    Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves\n",
    "    树中终端节点或叶子的最大数量。  \n",
    "    \n",
    "    可以在max_depth的位置定义。自创建二叉树的深度“n”会产生最多2 ^ n树叶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么时候使用树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "\n",
    "\n",
    "这取决于要解决的问题，如果特征和预测变量之间是线性关系，那么就使用线性回归模型  \n",
    "\n",
    "\n",
    "\n",
    "如果之间是非线性或者是更复杂的关系，那么树模型更合适  \n",
    "  \n",
    "\n",
    "如果模型需要有解释性，那么树模型比线性模型更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于树模型的集成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差： 从相同的人群中抽取不同的样本，所建模型在同一点的预测会有多大不同  \n",
    "\n",
    "偏差：平均预测值与实际值之间的差异有多大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型复杂度增加时，偏差减小，方差会增大。足够复杂时，会有高方差，既过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging可以减少方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 集成步骤\n",
    "\n",
    "1. 放回重抽样。 Bootstrap    \n",
    "\n",
    "2.基于子集建立模型    \n",
    "\n",
    "3.模型预测结果平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成模型比子模型更鲁棒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论上，模型的数量是越多越好。增加一个模型，方差缩减原方差的1/n.（此处不考虑选择性集成）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于bagging的决策树集成算法，将多个弱学习器组合成一个强学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成多棵决策树，分类情况下使用投票法，回归情况下，取各个树的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不放回的随机抽取训练子集，  \n",
    "\n",
    "在子集上选择m个特征(如果有M输入变量，m < M)，m是在M随机选取的，每个split node上选择其中一个为最佳分割点。  \n",
    "\n",
    "每棵树都尽可能地生长，不需要修剪。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林优势：  \n",
    "\n",
    "即可以处理分类问题也可以处理回归问题  \n",
    "\n",
    "可以做特征选择   \n",
    "\n",
    "可以处理高维特征  \n",
    "  \n",
    "可以输出特征的重要性    \n",
    "\n",
    "即便有大量缺失数据，也可以保持较高的准确度  \n",
    "\n",
    "可以处理不平衡数据   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   随机森林的缺点：  \n",
    " 回归问题上不能预测超出训练数据的关系，如果有噪声容易过拟合  \n",
    "  \n",
    " 解释性较低，只能调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=1000)\n",
    "# model.fit(X,y)\n",
    "# y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting在中文意思是'提升',我的理解是通过一系列的操作将弱学习器转化为强学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一开始， 在学习器中每个样本有同样的权重。如果预测错误，或者误差偏大，那么在下次学习时会给与更高的权重。直到精度达到或者模型训练结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：  \n",
    "1.有正则项抑制过拟合  \n",
    "2.并行处理，比GBM更快，  \n",
    "3.过度灵活。可以定义各种目标函数和评价准则  \n",
    "4.可以内部处理缺失值   \n",
    "5.Tree Pruning  . 先生成到max_depth然后剪枝，减去gain不是正值的支。  \n",
    "6.内部使用CV\n",
    "7.XGBoost允许用户在每次boost过程的迭代中运行交叉验证，因此很容易在一次运行中获得精确的最佳boost迭代次数。\n",
    "这与GBM不同，GBM需要进行网格搜索，只能测试有限的值。  \n",
    "\n",
    "7.用户可以从上一次运行的XGBoost模型的最后一次迭代开始训练XGBoost模型。这在某些特定的应用程序中具有显著的优势。\n",
    "sklearn的GBM实现也有这个特性，所以他们在这一点上是平等的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider the important GBM parameters used to improve model performance in Python:\n",
    "\n",
    "learning_rate：\n",
    "This determines the impact of each tree on the final outcome (step 2.4). GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n",
    "Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n",
    "Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n",
    "\n",
    "\n",
    "n_estimators:  \n",
    "\n",
    "The number of sequential trees to be modeled (step 2)\n",
    "Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n",
    "subsample\n",
    "The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "Values slightly less than 1 make the model robust by reducing the variance.\n",
    "Typical values ~0.8 generally work fine but can be fine-tuned further.\n",
    "Apart from these, there are certain miscellaneous parameters which affect overall functionality:\n",
    "\n",
    "loss:  \n",
    "\n",
    "It refers to the loss function to be minimized in each split.\n",
    "It can have various values for classification and regression case. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model.  \n",
    "\n",
    "init:  \n",
    "\n",
    "This affects initialization of the output.\n",
    "This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n",
    "random_state:  \n",
    "\n",
    "The random number seed so that same random numbers are generated every time.\n",
    "This is important for parameter tuning. If we don’t fix the random number, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.\n",
    "It can potentially result in overfitting to a particular random sample selected. We can try running models for different random samples, which is computationally expensive and generally not used.  \n",
    "\n",
    "verbose:  \n",
    "\n",
    "The type of output to be printed when the model fits. The different values can be:\n",
    "0: no output generated (default)\n",
    "1: output generated for trees in certain intervals\n",
    "1: output generated for all trees\n",
    "warm_start:  \n",
    "\n",
    "This parameter has an interesting application and can help a lot if used judicially.\n",
    "Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications\n",
    "presort :  \n",
    "\n",
    " Select whether to presort data for faster splits.\n",
    "It makes the selection automatically by default but it can be changed if needed.\n",
    "I know its a long list of parameters but I have simplified it for you in an excel file which you can download from this GitHub repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Boosting:  \n",
    "    1.Adaboost  \n",
    "    2.Gradient Tree Boosting\n",
    "    3.XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting: 适用于大量数据的情形，用于增强算法的子模型可以是任意模型。Adaboost是基于决策树的集成算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
